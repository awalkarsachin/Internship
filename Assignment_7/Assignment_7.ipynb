{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2c3b32",
   "metadata": {},
   "source": [
    "# Assignment 7 (Dated - 25.02.2024)\n",
    "\n",
    "### Name - Sachin Subhash Awalkar\n",
    "### Batch Code - DS2312\n",
    "### Subject - Web Scraping Assignment - 4\n",
    "### Submited on - 01.03.2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5eba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as re\n",
    "import re\n",
    "import urllib\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c9d0e",
   "metadata": {},
   "source": [
    "## 1. Scrape the details of most viewed videos on YouTube from Wikipedia. \n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "\n",
    "### You need to find following details:\n",
    "    A) Rank\n",
    "    B) Name\n",
    "    C) Artist\n",
    "    D) Upload date\n",
    "    E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf856c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdd8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dfd40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['\"Baby Shark Dance\"[6]', '\"Despacito\"[9]', '\"Johny Johny Yes Papa\"[17]', '\"Bath Song\"[18]', '\"Shape of You\"[19]', '\"See You Again\"[22]', '\"Wheels on the Bus\"[27]', '\"Phonics Song with Two Words\"[28]', '\"Uptown Funk\"[29]', '\"Learning Colors – Colorful Eggs on a Farm\"[30]', '\"Gangnam Style\"[31]', '\"Masha and the Bear – Recipe for Disaster\"[36]', '\"Dame Tu Cosita\"[37]', '\"Axel F\"[38]', '\"Sugar\"[39]', '\"Counting Stars\"[40]', '\"Baa Baa Black Sheep\"[41]', '\"Roar\"[42]', '\"Lakdi Ki Kathi\"[43]', '\"Waka Waka (This Time for Africa)\"[44]', '\"Sorry\"[45]', '\"Thinking Out Loud\"[46]', '\"Humpty the train on a fruits ride\"[47]', '\"Shree Hanuman Chalisa\"[48]', '\"Dark Horse\"[49]', '\"Perfect\"[50]', '\"Let Her Go\"[51]', '\"Faded\"[52]', '\"Girls Like You\"[53]', '\"Lean On\"[54]']\n",
      "30 [\"Pinkfong Baby Shark - Kids' Songs & Stories\", 'Luis Fonsi', \"LooLoo Kids - Nursery Rhymes and Children's Songs\", 'Cocomelon - Nursery Rhymes', 'Ed Sheeran', 'Wiz Khalifa', 'Cocomelon - Nursery Rhymes', 'ChuChu TV Nursery Rhymes & Kids Songs', 'Mark Ronson', 'Miroshka TV', 'Psy', 'Get Movies', 'Ultra Records', 'Crazy Frog', 'Maroon 5', 'OneRepublic', 'Cocomelon - Nursery Rhymes', 'Katy Perry', 'Jingle Toons', 'Shakira', 'Justin Bieber', 'Ed Sheeran', 'Kiddiestv Hindi - Nursery Rhymes & Kids Songs', 'T-Series Bhakti Sagar', 'Katy Perry', 'Ed Sheeran', 'Passenger', 'Alan Walker', 'Maroon 5', 'Major Lazer Official']\n",
      "30 ['14.09', '8.38', '6.87', '6.62', '6.20', '6.17', '5.88', '5.70', '5.15', '5.07', '5.05', '4.58', '4.55', '4.34', '4.00', '3.97', '3.96', '3.96', '3.91', '3.85', '3.77', '3.73', '3.73', '3.69', '3.67', '3.67', '3.61', '3.59', '3.56', '3.55']\n",
      "30 ['June 17, 2016', 'January 12, 2017', 'October 8, 2016', 'May 2, 2018', 'January 30, 2017', 'April 6, 2015', 'May 24, 2018', 'March 6, 2014', 'November 19, 2014', 'February 27, 2018', 'July 15, 2012', 'January 31, 2012', 'April 5, 2018', 'June 16, 2009', 'January 14, 2015', 'May 31, 2013', 'June 25, 2018', 'September 5, 2013', 'June 14, 2018', 'June 4, 2010', 'October 22, 2015', 'October 7, 2014', 'January 26, 2018', 'May 10, 2011', 'February 20, 2014', 'November 9, 2017', 'July 25, 2012', 'December 3, 2015', 'May 31, 2018', 'March 22, 2015']\n"
     ]
    }
   ],
   "source": [
    "#rank = []\n",
    "name = []\n",
    "artist = []\n",
    "upload_date = []\n",
    "views = []\n",
    "\n",
    "#rank_tags = driver.find_elements(By.XPATH,'//*[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr')\n",
    "#for i in rank_tags:\n",
    "#    try:\n",
    "#        rank.append(i.text)\n",
    "#    except:\n",
    "#        rank.append('-')\n",
    "#print(len(rank),rank)\n",
    "\n",
    "name_tags = driver.find_elements(By.XPATH,'//*[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[1]')\n",
    "for i in name_tags:\n",
    "    try:\n",
    "        name.append(i.text)\n",
    "    except:\n",
    "        name.append('-')\n",
    "print(len(name),name)\n",
    "\n",
    "artist_tags = driver.find_elements(By.XPATH,'//*[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[2]')\n",
    "for i in artist_tags:\n",
    "    try:\n",
    "        artist.append(i.text)\n",
    "    except:\n",
    "        artist.append('-')\n",
    "print(len(artist),artist)\n",
    "\n",
    "upload_tags = driver.find_elements(By.XPATH,'//*[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[3]')\n",
    "for i in upload_tags:\n",
    "    try:\n",
    "        upload_date.append(i.text)\n",
    "    except:\n",
    "        upload_date.append('-')\n",
    "print(len(upload_date),upload_date)\n",
    "\n",
    "view_tags = driver.find_elements(By.XPATH,'//*[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[4]')\n",
    "for i in view_tags:\n",
    "    try:\n",
    "        views.append(i.text)\n",
    "    except:\n",
    "        views.append('-')\n",
    "print(len(views),views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442e0cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Most Viewed Videos on YouTube =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>14.09</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.38</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Johny Johny Yes Papa\"[17]</td>\n",
       "      <td>LooLoo Kids - Nursery Rhymes and Children's Songs</td>\n",
       "      <td>6.87</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bath Song\"[18]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>6.62</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Shape of You\"[19]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>6.20</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"See You Again\"[22]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>6.17</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"Wheels on the Bus\"[27]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>5.88</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Phonics Song with Two Words\"[28]</td>\n",
       "      <td>ChuChu TV Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>5.70</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Uptown Funk\"[29]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>5.15</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[30]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>5.07</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Gangnam Style\"[31]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>5.05</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[36]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.58</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Dame Tu Cosita\"[37]</td>\n",
       "      <td>Ultra Records</td>\n",
       "      <td>4.55</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Axel F\"[38]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>4.34</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Sugar\"[39]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Counting Stars\"[40]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.97</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>3.96</td>\n",
       "      <td>June 25, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"Roar\"[42]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.96</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Lakdi Ki Kathi\"[43]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>3.91</td>\n",
       "      <td>June 14, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[44]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.85</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"Sorry\"[45]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.77</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"Thinking Out Loud\"[46]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.73</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Humpty the train on a fruits ride\"[47]</td>\n",
       "      <td>Kiddiestv Hindi - Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>3.73</td>\n",
       "      <td>January 26, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Shree Hanuman Chalisa\"[48]</td>\n",
       "      <td>T-Series Bhakti Sagar</td>\n",
       "      <td>3.69</td>\n",
       "      <td>May 10, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"Dark Horse\"[49]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.67</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\"Perfect\"[50]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.67</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"Let Her Go\"[51]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.61</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"Faded\"[52]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.59</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"Girls Like You\"[53]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.56</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"Lean On\"[54]</td>\n",
       "      <td>Major Lazer Official</td>\n",
       "      <td>3.55</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Name  \\\n",
       "0                             \"Baby Shark Dance\"[6]   \n",
       "1                                    \"Despacito\"[9]   \n",
       "2                        \"Johny Johny Yes Papa\"[17]   \n",
       "3                                   \"Bath Song\"[18]   \n",
       "4                                \"Shape of You\"[19]   \n",
       "5                               \"See You Again\"[22]   \n",
       "6                           \"Wheels on the Bus\"[27]   \n",
       "7                 \"Phonics Song with Two Words\"[28]   \n",
       "8                                 \"Uptown Funk\"[29]   \n",
       "9   \"Learning Colors – Colorful Eggs on a Farm\"[30]   \n",
       "10                              \"Gangnam Style\"[31]   \n",
       "11   \"Masha and the Bear – Recipe for Disaster\"[36]   \n",
       "12                             \"Dame Tu Cosita\"[37]   \n",
       "13                                     \"Axel F\"[38]   \n",
       "14                                      \"Sugar\"[39]   \n",
       "15                             \"Counting Stars\"[40]   \n",
       "16                        \"Baa Baa Black Sheep\"[41]   \n",
       "17                                       \"Roar\"[42]   \n",
       "18                             \"Lakdi Ki Kathi\"[43]   \n",
       "19           \"Waka Waka (This Time for Africa)\"[44]   \n",
       "20                                      \"Sorry\"[45]   \n",
       "21                          \"Thinking Out Loud\"[46]   \n",
       "22          \"Humpty the train on a fruits ride\"[47]   \n",
       "23                      \"Shree Hanuman Chalisa\"[48]   \n",
       "24                                 \"Dark Horse\"[49]   \n",
       "25                                    \"Perfect\"[50]   \n",
       "26                                 \"Let Her Go\"[51]   \n",
       "27                                      \"Faded\"[52]   \n",
       "28                             \"Girls Like You\"[53]   \n",
       "29                                    \"Lean On\"[54]   \n",
       "\n",
       "                                               Artist Upload Date  \\\n",
       "0         Pinkfong Baby Shark - Kids' Songs & Stories       14.09   \n",
       "1                                          Luis Fonsi        8.38   \n",
       "2   LooLoo Kids - Nursery Rhymes and Children's Songs        6.87   \n",
       "3                          Cocomelon - Nursery Rhymes        6.62   \n",
       "4                                          Ed Sheeran        6.20   \n",
       "5                                         Wiz Khalifa        6.17   \n",
       "6                          Cocomelon - Nursery Rhymes        5.88   \n",
       "7               ChuChu TV Nursery Rhymes & Kids Songs        5.70   \n",
       "8                                         Mark Ronson        5.15   \n",
       "9                                         Miroshka TV        5.07   \n",
       "10                                                Psy        5.05   \n",
       "11                                         Get Movies        4.58   \n",
       "12                                      Ultra Records        4.55   \n",
       "13                                         Crazy Frog        4.34   \n",
       "14                                           Maroon 5        4.00   \n",
       "15                                        OneRepublic        3.97   \n",
       "16                         Cocomelon - Nursery Rhymes        3.96   \n",
       "17                                         Katy Perry        3.96   \n",
       "18                                       Jingle Toons        3.91   \n",
       "19                                            Shakira        3.85   \n",
       "20                                      Justin Bieber        3.77   \n",
       "21                                         Ed Sheeran        3.73   \n",
       "22      Kiddiestv Hindi - Nursery Rhymes & Kids Songs        3.73   \n",
       "23                              T-Series Bhakti Sagar        3.69   \n",
       "24                                         Katy Perry        3.67   \n",
       "25                                         Ed Sheeran        3.67   \n",
       "26                                          Passenger        3.61   \n",
       "27                                        Alan Walker        3.59   \n",
       "28                                           Maroon 5        3.56   \n",
       "29                               Major Lazer Official        3.55   \n",
       "\n",
       "                Views  \n",
       "0       June 17, 2016  \n",
       "1    January 12, 2017  \n",
       "2     October 8, 2016  \n",
       "3         May 2, 2018  \n",
       "4    January 30, 2017  \n",
       "5       April 6, 2015  \n",
       "6        May 24, 2018  \n",
       "7       March 6, 2014  \n",
       "8   November 19, 2014  \n",
       "9   February 27, 2018  \n",
       "10      July 15, 2012  \n",
       "11   January 31, 2012  \n",
       "12      April 5, 2018  \n",
       "13      June 16, 2009  \n",
       "14   January 14, 2015  \n",
       "15       May 31, 2013  \n",
       "16      June 25, 2018  \n",
       "17  September 5, 2013  \n",
       "18      June 14, 2018  \n",
       "19       June 4, 2010  \n",
       "20   October 22, 2015  \n",
       "21    October 7, 2014  \n",
       "22   January 26, 2018  \n",
       "23       May 10, 2011  \n",
       "24  February 20, 2014  \n",
       "25   November 9, 2017  \n",
       "26      July 25, 2012  \n",
       "27   December 3, 2015  \n",
       "28       May 31, 2018  \n",
       "29     March 22, 2015  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Most Viewed Videos on YouTube =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\")\n",
    "youtube = pd.DataFrame({'Name':name,\n",
    "                        'Artist':artist,\n",
    "                        'Upload Date':upload_date,\n",
    "                        'Views':views                        \n",
    "                       })\n",
    "youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd5cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5365c25d",
   "metadata": {},
   "source": [
    "## 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "\n",
    "#### You need to find following details:\n",
    "    A) Series\n",
    "    B) Place\n",
    "    C) Date\n",
    "    D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229972aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "url = 'https://www.bcci.tv/'\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd5af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixtures = driver.find_element(By.XPATH,\"/html/body/header/div[3]/div[2]/ul/div[1]/a[2]\").click()\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9646a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['ENGLAND TOUR OF INDIA 2023-24', 'INDIA TOUR OF ZIMBABWE 2024', 'INDIA TOUR OF ZIMBABWE 2024', 'INDIA TOUR OF ZIMBABWE 2024', 'INDIA TOUR OF ZIMBABWE 2024', 'INDIA TOUR OF ZIMBABWE 2024']\n",
      "6 ['Himachal Pradesh Cricket Association Stadium, Dharamsala', 'Harare Sports Club, Harare', 'Harare Sports Club, Harare', 'Harare Sports Club, Harare', 'Harare Sports Club, Harare', 'Harare Sports Club, Harare']\n",
      "6 ['7 MARCH, 2024', '6 JULY, 2024', '7 JULY, 2024', '10 JULY, 2024', '13 JULY, 2024', '14 JULY, 2024']\n",
      "6 ['9:30 AM IST', '8:00 PM IST', '8:00 PM IST', '8:00 PM IST', '8:00 PM IST', '8:00 PM IST']\n"
     ]
    }
   ],
   "source": [
    "series = []\n",
    "place = []\n",
    "date = []\n",
    "time = []\n",
    "\n",
    "series_tags = driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "for i in series_tags:\n",
    "    try:\n",
    "        series.append(i.text)\n",
    "    except:\n",
    "        series.append('-')\n",
    "print(len(series),series)\n",
    "\n",
    "place_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-place ng-scope\"]')\n",
    "for i in place_tags:\n",
    "    try:\n",
    "        place.append(i.text)\n",
    "    except:\n",
    "        place.append('-')\n",
    "print(len(place),place)\n",
    "\n",
    "date_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')\n",
    "for i in date_tags:\n",
    "    try:\n",
    "        date.append(i.text)\n",
    "    except:\n",
    "        date.append('-')\n",
    "print(len(date),date)\n",
    "\n",
    "time_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')\n",
    "for i in time_tags:\n",
    "    try:\n",
    "        time.append(i.text)\n",
    "    except:\n",
    "        time.append('-')\n",
    "print(len(time),time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2d491e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Team India's International Fixtures =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>Himachal Pradesh Cricket Association Stadium, ...</td>\n",
       "      <td>7 MARCH, 2024</td>\n",
       "      <td>9:30 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>6 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>7 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>10 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>13 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>14 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Series  \\\n",
       "0  ENGLAND TOUR OF INDIA 2023-24   \n",
       "1    INDIA TOUR OF ZIMBABWE 2024   \n",
       "2    INDIA TOUR OF ZIMBABWE 2024   \n",
       "3    INDIA TOUR OF ZIMBABWE 2024   \n",
       "4    INDIA TOUR OF ZIMBABWE 2024   \n",
       "5    INDIA TOUR OF ZIMBABWE 2024   \n",
       "\n",
       "                                               Place           Date  \\\n",
       "0  Himachal Pradesh Cricket Association Stadium, ...  7 MARCH, 2024   \n",
       "1                         Harare Sports Club, Harare   6 JULY, 2024   \n",
       "2                         Harare Sports Club, Harare   7 JULY, 2024   \n",
       "3                         Harare Sports Club, Harare  10 JULY, 2024   \n",
       "4                         Harare Sports Club, Harare  13 JULY, 2024   \n",
       "5                         Harare Sports Club, Harare  14 JULY, 2024   \n",
       "\n",
       "          Time  \n",
       "0  9:30 AM IST  \n",
       "1  8:00 PM IST  \n",
       "2  8:00 PM IST  \n",
       "3  8:00 PM IST  \n",
       "4  8:00 PM IST  \n",
       "5  8:00 PM IST  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Team India's International Fixtures =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\")\n",
    "df = pd.DataFrame({'Series':series,\n",
    "                   'Place':place,\n",
    "                   'Date':date,\n",
    "                   'Time':time\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f561e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fb9ae54",
   "metadata": {},
   "source": [
    "## 3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "### You have to find following details:\n",
    "    A) Rank\n",
    "    B) State\n",
    "    C) GSDP(18-19)- at current prices\n",
    "    D) GSDP(19-20)- at current prices\n",
    "    E) Share(18-19)\n",
    "    F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c174f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "url = 'http://statisticstimes.com/'\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73be060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown = driver.find_element(By.XPATH,'//div[@class=\"dropdown\"][2]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c2e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_down = driver.find_element(By.XPATH,'//a[@href=\"economy/india-statistics.php\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b70a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "statewise_gdp = driver.find_element(By.XPATH,'//a[@href=\"india/indian-states-gdp.php\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94cbc2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n",
      "33 ['Maharashtra', 'Tamil Nadu', 'Uttar Pradesh', 'Karnataka', 'Gujarat', 'West Bengal', 'Rajasthan', 'Madhya Pradesh', 'Andhra Pradesh', 'Telangana', 'Kerala', 'Delhi', 'Haryana', 'Odisha', 'Bihar', 'Punjab', 'Assam', 'Chhattisgarh', 'Jharkhand', 'Uttarakhand', 'Jammu & Kashmir-UT', 'Himachal Pradesh', 'Goa', 'Tripura', 'Chandigarh', 'Puducherry', 'Meghalaya', 'Sikkim', 'Manipur', 'Arunachal Pradesh', 'Nagaland', 'Mizoram', 'Andaman & Nicobar Islands']\n",
      "33 ['3,108,022', '2,071,286', '1,974,532', '1,962,725', '1,937,066', '1,363,926', '1,218,193', '1,136,137', '1,133,837', '1,128,907', '932,470', '904,642', '870,665', '670,881', '650,302', '614,227', '412,612', '406,416', '358,863', '272,159', '199,917', '176,269', '82,604', '62,550', '45,635', '44,238', '38,785', '37,557', '36,594', '35,124', '31,913', '27,824', '10,371']\n",
      "33 ['-', '2,364,514', '2,257,575', '2,241,368', '-', '1,554,992', '1,413,620', '1,322,821', '1,317,728', '1,313,391', '-', '1,043,759', '994,154', '774,869', '751,396', '673,107', '493,167', '457,608', '393,722', '302,621', '227,927', '195,405', '-', '72,636', '-', '-', '42,697', '42,756', '-', '-', '-', '-', '-']\n",
      "33 ['13.24%', '8.82%', '8.41%', '8.36%', '8.25%', '5.81%', '5.19%', '4.84%', '4.83%', '4.81%', '3.97%', '3.85%', '3.71%', '2.86%', '2.77%', '2.62%', '1.76%', '1.73%', '1.53%', '1.16%', '0.85%', '0.75%', '0.35%', '0.27%', '0.19%', '0.19%', '0.17%', '0.16%', '0.16%', '0.15%', '0.14%', '0.12%', '0.04%']\n",
      "33 ['417.163', '278.011', '265.024', '263.440', '259.996', '183.068', '163.507', '152.494', '152.185', '151.523', '125.157', '121.422', '116.862', '90.047', '87.284', '82.442', '55.381', '54.550', '48.167', '36.530', '26.833', '23.659', '11.087', '8.396', '6.125', '5.938', '5.206', '5.041', '4.912', '4.714', '4.283', '3.735', '1.392']\n"
     ]
    }
   ],
   "source": [
    "rank = []\n",
    "rank_tags = driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "for i in rank_tags:\n",
    "    try:\n",
    "        rank.append(i.text)\n",
    "    except:\n",
    "        rank.append('-')\n",
    "print(len(rank),rank)\n",
    "\n",
    "state = []\n",
    "state_tags = driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "for i in state_tags:\n",
    "    try:\n",
    "        state.append(i.text)\n",
    "    except:\n",
    "        state.append('-')\n",
    "print(len(state),state)\n",
    "\n",
    "GSDP_21_22 = []\n",
    "GSDP_21_22_tags = driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "for i in GSDP_21_22_tags:\n",
    "    try:\n",
    "        GSDP_21_22.append(i.text)\n",
    "    except:\n",
    "        GSDP_21_22.append('-')\n",
    "print(len(GSDP_21_22),GSDP_21_22)\n",
    "\n",
    "GSDP_22_23 = []\n",
    "GSDP_22_23_tags = driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "for i in GSDP_22_23_tags:\n",
    "    try:\n",
    "        GSDP_22_23.append(i.text)\n",
    "    except:\n",
    "        GSDP_22_23.append('-')\n",
    "print(len(GSDP_22_23),GSDP_22_23)\n",
    "\n",
    "share = []\n",
    "share_tags = driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "for i in share_tags:\n",
    "    try:\n",
    "        share.append(i.text)\n",
    "    except:\n",
    "        share.append('-')\n",
    "print(len(share),share)\n",
    "\n",
    "GDP_Billion = []\n",
    "GDP_tags = driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "for i in GDP_tags:\n",
    "    try:\n",
    "        GDP_Billion.append(i.text)\n",
    "    except:\n",
    "        GDP_Billion.append('-')\n",
    "print(len(GDP_Billion),GDP_Billion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b53399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= State-wise GDP of India =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP (21-22)</th>\n",
       "      <th>GSDP (22-23)</th>\n",
       "      <th>Share (21-22)</th>\n",
       "      <th>GDP ($ Billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>3,108,022</td>\n",
       "      <td>-</td>\n",
       "      <td>13.24%</td>\n",
       "      <td>417.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>2,071,286</td>\n",
       "      <td>2,364,514</td>\n",
       "      <td>8.82%</td>\n",
       "      <td>278.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,974,532</td>\n",
       "      <td>2,257,575</td>\n",
       "      <td>8.41%</td>\n",
       "      <td>265.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,962,725</td>\n",
       "      <td>2,241,368</td>\n",
       "      <td>8.36%</td>\n",
       "      <td>263.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,937,066</td>\n",
       "      <td>-</td>\n",
       "      <td>8.25%</td>\n",
       "      <td>259.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,363,926</td>\n",
       "      <td>1,554,992</td>\n",
       "      <td>5.81%</td>\n",
       "      <td>183.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,218,193</td>\n",
       "      <td>1,413,620</td>\n",
       "      <td>5.19%</td>\n",
       "      <td>163.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>1,136,137</td>\n",
       "      <td>1,322,821</td>\n",
       "      <td>4.84%</td>\n",
       "      <td>152.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>1,133,837</td>\n",
       "      <td>1,317,728</td>\n",
       "      <td>4.83%</td>\n",
       "      <td>152.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>1,128,907</td>\n",
       "      <td>1,313,391</td>\n",
       "      <td>4.81%</td>\n",
       "      <td>151.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>932,470</td>\n",
       "      <td>-</td>\n",
       "      <td>3.97%</td>\n",
       "      <td>125.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>904,642</td>\n",
       "      <td>1,043,759</td>\n",
       "      <td>3.85%</td>\n",
       "      <td>121.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>870,665</td>\n",
       "      <td>994,154</td>\n",
       "      <td>3.71%</td>\n",
       "      <td>116.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>670,881</td>\n",
       "      <td>774,869</td>\n",
       "      <td>2.86%</td>\n",
       "      <td>90.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>650,302</td>\n",
       "      <td>751,396</td>\n",
       "      <td>2.77%</td>\n",
       "      <td>87.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>614,227</td>\n",
       "      <td>673,107</td>\n",
       "      <td>2.62%</td>\n",
       "      <td>82.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>412,612</td>\n",
       "      <td>493,167</td>\n",
       "      <td>1.76%</td>\n",
       "      <td>55.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>406,416</td>\n",
       "      <td>457,608</td>\n",
       "      <td>1.73%</td>\n",
       "      <td>54.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>358,863</td>\n",
       "      <td>393,722</td>\n",
       "      <td>1.53%</td>\n",
       "      <td>48.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>272,159</td>\n",
       "      <td>302,621</td>\n",
       "      <td>1.16%</td>\n",
       "      <td>36.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir-UT</td>\n",
       "      <td>199,917</td>\n",
       "      <td>227,927</td>\n",
       "      <td>0.85%</td>\n",
       "      <td>26.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>176,269</td>\n",
       "      <td>195,405</td>\n",
       "      <td>0.75%</td>\n",
       "      <td>23.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>82,604</td>\n",
       "      <td>-</td>\n",
       "      <td>0.35%</td>\n",
       "      <td>11.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>62,550</td>\n",
       "      <td>72,636</td>\n",
       "      <td>0.27%</td>\n",
       "      <td>8.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>45,635</td>\n",
       "      <td>-</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>6.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>44,238</td>\n",
       "      <td>-</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>5.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>38,785</td>\n",
       "      <td>42,697</td>\n",
       "      <td>0.17%</td>\n",
       "      <td>5.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>37,557</td>\n",
       "      <td>42,756</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>5.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>36,594</td>\n",
       "      <td>-</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>4.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>35,124</td>\n",
       "      <td>-</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>31,913</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>27,824</td>\n",
       "      <td>-</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>10,371</td>\n",
       "      <td>-</td>\n",
       "      <td>0.04%</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP (21-22) GSDP (22-23) Share (21-22)  \\\n",
       "0     1                Maharashtra    3,108,022            -        13.24%   \n",
       "1     2                 Tamil Nadu    2,071,286    2,364,514         8.82%   \n",
       "2     3              Uttar Pradesh    1,974,532    2,257,575         8.41%   \n",
       "3     4                  Karnataka    1,962,725    2,241,368         8.36%   \n",
       "4     5                    Gujarat    1,937,066            -         8.25%   \n",
       "5     6                West Bengal    1,363,926    1,554,992         5.81%   \n",
       "6     7                  Rajasthan    1,218,193    1,413,620         5.19%   \n",
       "7     8             Madhya Pradesh    1,136,137    1,322,821         4.84%   \n",
       "8     9             Andhra Pradesh    1,133,837    1,317,728         4.83%   \n",
       "9    10                  Telangana    1,128,907    1,313,391         4.81%   \n",
       "10   11                     Kerala      932,470            -         3.97%   \n",
       "11   12                      Delhi      904,642    1,043,759         3.85%   \n",
       "12   13                    Haryana      870,665      994,154         3.71%   \n",
       "13   14                     Odisha      670,881      774,869         2.86%   \n",
       "14   15                      Bihar      650,302      751,396         2.77%   \n",
       "15   16                     Punjab      614,227      673,107         2.62%   \n",
       "16   17                      Assam      412,612      493,167         1.76%   \n",
       "17   18               Chhattisgarh      406,416      457,608         1.73%   \n",
       "18   19                  Jharkhand      358,863      393,722         1.53%   \n",
       "19   20                Uttarakhand      272,159      302,621         1.16%   \n",
       "20   21         Jammu & Kashmir-UT      199,917      227,927         0.85%   \n",
       "21   22           Himachal Pradesh      176,269      195,405         0.75%   \n",
       "22   23                        Goa       82,604            -         0.35%   \n",
       "23   24                    Tripura       62,550       72,636         0.27%   \n",
       "24   25                 Chandigarh       45,635            -         0.19%   \n",
       "25   26                 Puducherry       44,238            -         0.19%   \n",
       "26   27                  Meghalaya       38,785       42,697         0.17%   \n",
       "27   28                     Sikkim       37,557       42,756         0.16%   \n",
       "28   29                    Manipur       36,594            -         0.16%   \n",
       "29   30          Arunachal Pradesh       35,124            -         0.15%   \n",
       "30   31                   Nagaland       31,913            -         0.14%   \n",
       "31   32                    Mizoram       27,824            -         0.12%   \n",
       "32   33  Andaman & Nicobar Islands       10,371            -         0.04%   \n",
       "\n",
       "   GDP ($ Billion)  \n",
       "0          417.163  \n",
       "1          278.011  \n",
       "2          265.024  \n",
       "3          263.440  \n",
       "4          259.996  \n",
       "5          183.068  \n",
       "6          163.507  \n",
       "7          152.494  \n",
       "8          152.185  \n",
       "9          151.523  \n",
       "10         125.157  \n",
       "11         121.422  \n",
       "12         116.862  \n",
       "13          90.047  \n",
       "14          87.284  \n",
       "15          82.442  \n",
       "16          55.381  \n",
       "17          54.550  \n",
       "18          48.167  \n",
       "19          36.530  \n",
       "20          26.833  \n",
       "21          23.659  \n",
       "22          11.087  \n",
       "23           8.396  \n",
       "24           6.125  \n",
       "25           5.938  \n",
       "26           5.206  \n",
       "27           5.041  \n",
       "28           4.912  \n",
       "29           4.714  \n",
       "30           4.283  \n",
       "31           3.735  \n",
       "32           1.392  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= State-wise GDP of India =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\")\n",
    "df = pd.DataFrame({'Rank':rank,\n",
    "                   'State':state,\n",
    "                   'GSDP (21-22)':GSDP_21_22,\n",
    "                   'GSDP (22-23)':GSDP_22_23,\n",
    "                   'Share (21-22)':share,\n",
    "                   'GDP ($ Billion)':GDP_Billion\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78fdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6907081f",
   "metadata": {},
   "source": [
    "## 4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "### You have to find the following details:\n",
    "    A) Repository title\n",
    "    B) Repository description\n",
    "    C) Contributors count\n",
    "    D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5064564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88096886",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_repo = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button').click()\n",
    "trending = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c98dce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 ['Lissy93 / web-check', 'yuzu-emu / yuzu', 'Netflix / bpftop', 'yuzu-emu / yuzu-android', 'dockur / windows', 'polyfillpolyfill / polyfill-service', 'taikoxyz / taiko-mono', 'Eladlev / AutoPrompt', 'public-apis / public-apis', 'Lunakepio / Mario-Kart-3.js', 'HumanAIGC / EMO', 'yuzu-emu / yuzu-mainline', 'SuperTux / supertux', '1c7 / chinese-independent-developer', 'toeverything / AFFiNE', 'HumanAIGC / AnimateAnyone', 'tigerbeetle / tigerbeetle', 'pineappleEA / pineapple-src', 'pure-admin / vue-pure-admin', 'Ryujinx / Ryujinx', 'Avaiga / taipy']\n",
      "21 ['🕵️\\u200d♂️ All-in-one OSINT tool for analysing any website', 'Nintendo Switch emulator', 'bpftop provides a dynamic real-time view of running eBPF programs. It displays the average runtime, events per second, and estimated total CPU % for each program.', 'Windows in a Docker container.', 'Automatic polyfill service.', 'A based rollup. 🥁', 'A framework for prompt tuning using Intent-based Prompt Calibration', 'A collective list of free APIs', 'SuperTux source code', '👩🏿\\u200d💻👨🏾\\u200d💻👩🏼\\u200d💻👨🏽\\u200d💻👩🏻\\u200d💻中国独立开发者项目列表 -- 分享大家都在做什么', 'There can be more than Notion and Miro. AFFiNE(pronounced [ə‘fain]) is a next-gen knowledge base that brings planning, sorting and creating all together. Privacy first, open-source, customizable and ready to use.', 'Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation', 'The distributed financial transactions database designed for mission critical safety and performance.', 'yuzu Early Access source code', '🔥 全面ESM+Vue3+Vite+Element-Plus+TypeScript编写的一款后台管理系统（兼容移动端）', 'Experimental Nintendo Switch Emulator written in C#', 'Turns Data and AI algorithms into production-ready web applications in no time.', 'A curated list of free courses & certifications.', '🎓 Path to a free self-taught education in Computer Science!', 'High-quality multi-lingual text-to-speech library by MyShell.ai. Support English, Spanish, French, Chinese, Japanese and Korean.', \"Let's bake some (AI) stickers!\"]\n",
      "21 ['888', '3,783', '42', '163', '178', '573', '1,585', '72', '30,970', '619', '232', '379', '436', '2,482', '1,801', '791', '278', '292', '2,418', '2,455', '420']\n",
      "21 ['TypeScript', 'C++', 'Rust', 'C++', 'Shell', 'Built by', 'HTML', 'Python', 'Python', 'JavaScript', 'Built by', 'C++', 'C++', 'Built by', 'TypeScript', 'Built by', 'Zig', 'C++', 'Vue', 'C#', 'Python']\n"
     ]
    }
   ],
   "source": [
    "repo_title = []\n",
    "title_tags = driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/h2/a')\n",
    "for i in title_tags[:21]:\n",
    "    try:\n",
    "        repo_title.append(i.text)\n",
    "    except:\n",
    "        repo_title.append('-')\n",
    "print(len(repo_title),repo_title)\n",
    "\n",
    "repo_desc = []\n",
    "desc_tags = driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/p')\n",
    "for i in desc_tags[:21]:\n",
    "    try:\n",
    "        repo_desc.append(i.text)\n",
    "    except:\n",
    "        repo_desc.append('-')\n",
    "print(len(repo_desc),repo_desc)\n",
    "\n",
    "cont = []\n",
    "cont_tags = driver.find_elements(By.XPATH,'//div[@class=\"f6 color-fg-muted mt-2\"]/a[2]')\n",
    "for i in cont_tags[:21]:\n",
    "    try:\n",
    "        cont.append(i.text)\n",
    "    except:\n",
    "        cont.append('-')\n",
    "print(len(cont),cont)\n",
    "\n",
    "lang = []\n",
    "lang_tags = driver.find_elements(By.XPATH,'//div[@class=\"f6 color-fg-muted mt-2\"]/span[1]')\n",
    "for i in lang_tags[:21]:\n",
    "    try:\n",
    "        lang.append(i.text)\n",
    "    except:\n",
    "        lang.append('-')\n",
    "print(len(lang),lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ee461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Trending Repositories =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors</th>\n",
       "      <th>Language Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lissy93 / web-check</td>\n",
       "      <td>🕵️‍♂️ All-in-one OSINT tool for analysing any ...</td>\n",
       "      <td>888</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yuzu-emu / yuzu</td>\n",
       "      <td>Nintendo Switch emulator</td>\n",
       "      <td>3,783</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netflix / bpftop</td>\n",
       "      <td>bpftop provides a dynamic real-time view of ru...</td>\n",
       "      <td>42</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yuzu-emu / yuzu-android</td>\n",
       "      <td>Windows in a Docker container.</td>\n",
       "      <td>163</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dockur / windows</td>\n",
       "      <td>Automatic polyfill service.</td>\n",
       "      <td>178</td>\n",
       "      <td>Shell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>polyfillpolyfill / polyfill-service</td>\n",
       "      <td>A based rollup. 🥁</td>\n",
       "      <td>573</td>\n",
       "      <td>Built by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>taikoxyz / taiko-mono</td>\n",
       "      <td>A framework for prompt tuning using Intent-bas...</td>\n",
       "      <td>1,585</td>\n",
       "      <td>HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eladlev / AutoPrompt</td>\n",
       "      <td>A collective list of free APIs</td>\n",
       "      <td>72</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>public-apis / public-apis</td>\n",
       "      <td>SuperTux source code</td>\n",
       "      <td>30,970</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lunakepio / Mario-Kart-3.js</td>\n",
       "      <td>👩🏿‍💻👨🏾‍💻👩🏼‍💻👨🏽‍💻👩🏻‍💻中国独立开发者项目列表 -- 分享大家都在做什么</td>\n",
       "      <td>619</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HumanAIGC / EMO</td>\n",
       "      <td>There can be more than Notion and Miro. AFFiNE...</td>\n",
       "      <td>232</td>\n",
       "      <td>Built by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yuzu-emu / yuzu-mainline</td>\n",
       "      <td>Animate Anyone: Consistent and Controllable Im...</td>\n",
       "      <td>379</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SuperTux / supertux</td>\n",
       "      <td>The distributed financial transactions databas...</td>\n",
       "      <td>436</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1c7 / chinese-independent-developer</td>\n",
       "      <td>yuzu Early Access source code</td>\n",
       "      <td>2,482</td>\n",
       "      <td>Built by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>toeverything / AFFiNE</td>\n",
       "      <td>🔥 全面ESM+Vue3+Vite+Element-Plus+TypeScript编写的一款...</td>\n",
       "      <td>1,801</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HumanAIGC / AnimateAnyone</td>\n",
       "      <td>Experimental Nintendo Switch Emulator written ...</td>\n",
       "      <td>791</td>\n",
       "      <td>Built by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tigerbeetle / tigerbeetle</td>\n",
       "      <td>Turns Data and AI algorithms into production-r...</td>\n",
       "      <td>278</td>\n",
       "      <td>Zig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pineappleEA / pineapple-src</td>\n",
       "      <td>A curated list of free courses &amp; certifications.</td>\n",
       "      <td>292</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pure-admin / vue-pure-admin</td>\n",
       "      <td>🎓 Path to a free self-taught education in Comp...</td>\n",
       "      <td>2,418</td>\n",
       "      <td>Vue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ryujinx / Ryujinx</td>\n",
       "      <td>High-quality multi-lingual text-to-speech libr...</td>\n",
       "      <td>2,455</td>\n",
       "      <td>C#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Avaiga / taipy</td>\n",
       "      <td>Let's bake some (AI) stickers!</td>\n",
       "      <td>420</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Repository Title  \\\n",
       "0                   Lissy93 / web-check   \n",
       "1                       yuzu-emu / yuzu   \n",
       "2                      Netflix / bpftop   \n",
       "3               yuzu-emu / yuzu-android   \n",
       "4                      dockur / windows   \n",
       "5   polyfillpolyfill / polyfill-service   \n",
       "6                 taikoxyz / taiko-mono   \n",
       "7                  Eladlev / AutoPrompt   \n",
       "8             public-apis / public-apis   \n",
       "9           Lunakepio / Mario-Kart-3.js   \n",
       "10                      HumanAIGC / EMO   \n",
       "11             yuzu-emu / yuzu-mainline   \n",
       "12                  SuperTux / supertux   \n",
       "13  1c7 / chinese-independent-developer   \n",
       "14                toeverything / AFFiNE   \n",
       "15            HumanAIGC / AnimateAnyone   \n",
       "16            tigerbeetle / tigerbeetle   \n",
       "17          pineappleEA / pineapple-src   \n",
       "18          pure-admin / vue-pure-admin   \n",
       "19                    Ryujinx / Ryujinx   \n",
       "20                       Avaiga / taipy   \n",
       "\n",
       "                               Repository Description Contributors  \\\n",
       "0   🕵️‍♂️ All-in-one OSINT tool for analysing any ...          888   \n",
       "1                            Nintendo Switch emulator        3,783   \n",
       "2   bpftop provides a dynamic real-time view of ru...           42   \n",
       "3                      Windows in a Docker container.          163   \n",
       "4                         Automatic polyfill service.          178   \n",
       "5                                   A based rollup. 🥁          573   \n",
       "6   A framework for prompt tuning using Intent-bas...        1,585   \n",
       "7                      A collective list of free APIs           72   \n",
       "8                                SuperTux source code       30,970   \n",
       "9        👩🏿‍💻👨🏾‍💻👩🏼‍💻👨🏽‍💻👩🏻‍💻中国独立开发者项目列表 -- 分享大家都在做什么          619   \n",
       "10  There can be more than Notion and Miro. AFFiNE...          232   \n",
       "11  Animate Anyone: Consistent and Controllable Im...          379   \n",
       "12  The distributed financial transactions databas...          436   \n",
       "13                      yuzu Early Access source code        2,482   \n",
       "14  🔥 全面ESM+Vue3+Vite+Element-Plus+TypeScript编写的一款...        1,801   \n",
       "15  Experimental Nintendo Switch Emulator written ...          791   \n",
       "16  Turns Data and AI algorithms into production-r...          278   \n",
       "17   A curated list of free courses & certifications.          292   \n",
       "18  🎓 Path to a free self-taught education in Comp...        2,418   \n",
       "19  High-quality multi-lingual text-to-speech libr...        2,455   \n",
       "20                     Let's bake some (AI) stickers!          420   \n",
       "\n",
       "   Language Used  \n",
       "0     TypeScript  \n",
       "1            C++  \n",
       "2           Rust  \n",
       "3            C++  \n",
       "4          Shell  \n",
       "5       Built by  \n",
       "6           HTML  \n",
       "7         Python  \n",
       "8         Python  \n",
       "9     JavaScript  \n",
       "10      Built by  \n",
       "11           C++  \n",
       "12           C++  \n",
       "13      Built by  \n",
       "14    TypeScript  \n",
       "15      Built by  \n",
       "16           Zig  \n",
       "17           C++  \n",
       "18           Vue  \n",
       "19            C#  \n",
       "20        Python  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Trending Repositories =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\")\n",
    "df = pd.DataFrame({'Repository Title':repo_title,\n",
    "                   'Repository Description':repo_desc,\n",
    "                   'Contributors':cont,\n",
    "                   'Language Used':lang\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc595a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8cf684b",
   "metadata": {},
   "source": [
    "## 5. Scrape the details of top 100 songs on billiboard.com. \n",
    "Url = https:/www.billboard.com/ \n",
    "### You have to find the following details:\n",
    "    A) Song name\n",
    "    B) Artist name\n",
    "    C) Last week rank\n",
    "    D) Peak rank\n",
    "    E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81839fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "url = \"https:/www.billboard.com/\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd7e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = driver.find_element(By.XPATH,'/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a').click()\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f01e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_100 = driver.find_element(By.XPATH,'/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[1]/div/div[2]/span/a').click()\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841a50fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [\"Texas Hold 'Em\", 'Lovin On Me', 'Lose Control', 'Carnival', 'Beautiful Things', 'Snooze', 'Cruel Summer', 'Greedy', 'I Remember Everything', 'Agora Hills', 'Yes, And?', 'Stick Season', 'Fast Car', 'Water', 'Last Night', 'Redrum', \"Thinkin' Bout Me\", \"Is It Over Now? (Taylor's Version) [From The Vault]\", 'Pretty Little Poison', 'Flowers', 'Paint The Town Red', 'Houdini', 'Made For Me', 'Never Lose Me', 'La Diabla', 'Where The Wild Things Are', 'Training Season', 'The Painter', 'Feather', 'What Was I Made For?', 'Rich Baby Daddy', 'Whatever She Wants', 'Truck Bed', 'Selfish', 'Everybody', 'Vampire', 'Wild Ones', 'On My Mama', 'Praise Jah In The Moonlight', 'Save Me', 'Exes', 'Breathe', 'Dance The Night', 'Need A Favor', 'Surround Sound', 'World On Fire', 'Yeah!', 'Burn It Down', 'Good Good', 'La Victima', 'End Of Beginning', 'Get In With Me', 'Burn', 'Man Made A Bar', '16 Carriages', 'First Person Shooter', 'Fuk Sumn', 'One Of The Girls', 'Back To Me', 'One Call', 'Contigo', '23', 'Murder On The Dancefloor', 'You Broke My Heart', 'Think U The Shit (Fart)', 'FTCU', 'FE!N', 'Hiss', 'I Can Feel It', 'Bandit', 'Spin You Around (1/24)', 'Soak City', 'Forever', \"Mamaw's House\", 'Act II: Date @ 8', 'Igual Que Un Angel', 'Nee-nah', 'Wildflowers And Wild Horses', 'Bellakeo', 'Harley Quinn', 'Standing Next To You', 'Bittersweet', 'IDGAF', 'Home', 'Vultures', 'Yeah Glo!', 'Coal', 'Mmhmm', 'Psycho CEO', 'Perro Negro', 'Sensational', 'Oklahoma Smokeshow', 'Tu Name', 'Do It', 'Worth It', 'Talking', 'Monaco', 'Where It Ends', 'Wondering Why', 'Northern Attitude']\n",
      "100 ['Beyonce', 'Jack Harlow', 'Teddy Swims', '¥$: Kanye West & Ty Dolla $ign Featuring Rich The Kid & Playboi Carti', 'Benson Boone', 'SZA', 'Taylor Swift', 'Tate McRae', 'Zach Bryan Featuring Kacey Musgraves', 'Doja Cat', 'Ariana Grande', 'Noah Kahan', 'Luke Combs', 'Tyla', 'Morgan Wallen', '21 Savage', 'Morgan Wallen', 'Taylor Swift', 'Warren Zeiders', 'Miley Cyrus', 'Doja Cat', 'Dua Lipa', 'Muni Long', 'Flo Milli', 'Xavi', 'Luke Combs', 'Dua Lipa', 'Cody Johnson', 'Sabrina Carpenter', 'Billie Eilish', 'Drake Featuring Sexyy Red & SZA', 'Bryson Tiller', 'HARDY', 'Justin Timberlake', 'Nicki Minaj Featuring Lil Uzi Vert', 'Olivia Rodrigo', 'Jessie Murph & Jelly Roll', 'Victoria Monet', 'YG Marley', 'Jelly Roll With Lainey Wilson', 'Tate McRae', 'Yeat', 'Dua Lipa', 'Jelly Roll', 'JID Featuring 21 Savage & Baby Tate', 'Nate Smith', 'Usher Featuring Lil Jon & Ludacris', 'Parker McCollum', 'Usher, Summer Walker & 21 Savage', 'Xavi', 'Djo', 'BossMan Dlow', '¥$: Kanye West & Ty Dolla $ign', 'Morgan Wallen Featuring Eric Church', 'Beyonce', 'Drake Featuring J. Cole', '¥$: Kanye West & Ty Dolla $ign', 'The Weeknd, Jennie & Lily Rose Depp', '¥$: Kanye West & Ty Dolla $ign', 'Rich Amiri', 'Karol G & Tiesto', 'Chayce Beckham', 'Sophie Ellis-Bextor', 'Drake', 'Ice Spice', 'Nicki Minaj', 'Travis Scott Featuring Playboi Carti', 'Megan Thee Stallion', 'Kane Brown', 'Don Toliver', 'Morgan Wallen', '310babii', 'Noah Kahan', 'Thomas Rhett Featuring Morgan Wallen', '4Batz', 'Kali Uchis & Peso Pluma', '21 Savage, Travis Scott & Metro Boomin', 'Lainey Wilson', 'Peso Pluma & Anitta', 'Fuerza Regida & Marshmello', 'Jung Kook', 'Gunna', 'Drake Featuring Yeat', 'Good Neighbours', '¥$: Kanye West & Ty Dolla $ign Featuring Lil Durk & Bump J', 'GloRilla', 'Dylan Gossett', 'BigXthaPlug', 'Yeat', 'Bad Bunny & Feid', 'Chris Brown Featuring Davido & Lojay', 'Zach Bryan', 'Fuerza Regida', '¥$: Kanye West & Ty Dolla $ign', 'Offset & Don Toliver', '¥$: Kanye West & Ty Dolla $ign Featuring North West', 'Bad Bunny', 'Bailey Zimmerman', 'The Red Clay Strays', 'Noah Kahan With Hozier']\n",
      "100 ['2', '1', '5', '3', '4', '7', '6', '9', '8', '12', '31', '10', '11', '13', '14', '15', '16', '18', '24', '17', '19', '25', '32', '21', '22', '29', '-', '35', '42', '27', '41', '-', '51', '37', '40', '43', '45', '44', '63', '47', '49', '-', '46', '50', '54', '56', '20', '60', '36', '58', '-', '59', '33', '66', '38', '57', '23', '72', '26', '68', '-', '77', '73', '74', '62', '71', '76', '48', '82', '78', '70', '84', '28', '83', '81', '69', '80', '-', '87', '85', '61', '-', '88', '-', '34', '89', '95', '96', '-', '99', '-', '-', '-', '52', '-', '30', '97', '-', '-', '75']\n",
      "100 ['1', '1', '2', '3', '3', '2', '1', '3', '1', '7', '1', '10', '2', '7', '1', '5', '7', '1', '19', '1', '1', '11', '23', '18', '20', '26', '27', '28', '29', '14', '11', '32', '33', '19', '24', '1', '35', '33', '39', '19', '34', '42', '6', '13', '40', '21', '1', '48', '25', '46', '51', '52', '33', '15', '38', '1', '23', '57', '26', '60', '61', '62', '51', '11', '37', '15', '5', '1', '59', '38', '24', '61', '28', '55', '59', '22', '10', '78', '53', '40', '5', '82', '2', '77', '34', '86', '86', '65', '89', '20', '91', '72', '93', '52', '92', '30', '5', '32', '71', '37']\n",
      "100 ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n"
     ]
    }
   ],
   "source": [
    "song_name = []\n",
    "song_tags = driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]//h3')\n",
    "for i in song_tags:\n",
    "    try:\n",
    "        song_name.append(i.text)\n",
    "    except:\n",
    "        song_name.append('-')\n",
    "print(len(song_name),song_name)\n",
    "\n",
    "artist_name = []\n",
    "artist_tags = driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[1]/span[1]')\n",
    "for i in artist_tags:\n",
    "    try:\n",
    "        artist_name.append(i.text)\n",
    "    except:\n",
    "        artist_name.append('-')\n",
    "print(len(artist_name),artist_name)\n",
    "\n",
    "lastW_rank = []\n",
    "lrank_tags = driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[4]/span')\n",
    "for i in lrank_tags:\n",
    "    try:\n",
    "        lastW_rank.append(i.text)\n",
    "    except:\n",
    "        lastW_rank.append('-')\n",
    "print(len(lastW_rank),lastW_rank)\n",
    "\n",
    "peak_rank = []\n",
    "Prank_tags = driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[5]/span')\n",
    "for i in Prank_tags:\n",
    "    try:\n",
    "        peak_rank.append(i.text)\n",
    "    except:\n",
    "        peak_rank.append('-')\n",
    "print(len(peak_rank),peak_rank)\n",
    "\n",
    "week_board = []\n",
    "week_tags = driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul[1]/li[1]/span[1]')\n",
    "for i in week_tags:\n",
    "    try:\n",
    "        week_board.append(i.text)\n",
    "    except:\n",
    "        week_board.append('-')\n",
    "print(len(week_board),week_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b52d456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Top 100 Songs =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on Board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas Hold 'Em</td>\n",
       "      <td>Beyonce</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lovin On Me</td>\n",
       "      <td>Jack Harlow</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lose Control</td>\n",
       "      <td>Teddy Swims</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carnival</td>\n",
       "      <td>¥$: Kanye West &amp; Ty Dolla $ign Featuring Rich ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beautiful Things</td>\n",
       "      <td>Benson Boone</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Talking</td>\n",
       "      <td>¥$: Kanye West &amp; Ty Dolla $ign Featuring North...</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Monaco</td>\n",
       "      <td>Bad Bunny</td>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Where It Ends</td>\n",
       "      <td>Bailey Zimmerman</td>\n",
       "      <td>-</td>\n",
       "      <td>32</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Wondering Why</td>\n",
       "      <td>The Red Clay Strays</td>\n",
       "      <td>-</td>\n",
       "      <td>71</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Northern Attitude</td>\n",
       "      <td>Noah Kahan With Hozier</td>\n",
       "      <td>75</td>\n",
       "      <td>37</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Song Name                                        Artist Name  \\\n",
       "0      Texas Hold 'Em                                            Beyonce   \n",
       "1         Lovin On Me                                        Jack Harlow   \n",
       "2        Lose Control                                        Teddy Swims   \n",
       "3            Carnival  ¥$: Kanye West & Ty Dolla $ign Featuring Rich ...   \n",
       "4    Beautiful Things                                       Benson Boone   \n",
       "..                ...                                                ...   \n",
       "95            Talking  ¥$: Kanye West & Ty Dolla $ign Featuring North...   \n",
       "96             Monaco                                          Bad Bunny   \n",
       "97      Where It Ends                                   Bailey Zimmerman   \n",
       "98      Wondering Why                                The Red Clay Strays   \n",
       "99  Northern Attitude                             Noah Kahan With Hozier   \n",
       "\n",
       "   Last Week Rank Peak Rank Weeks on Board  \n",
       "0               2         1              1  \n",
       "1               1         1              2  \n",
       "2               5         2              3  \n",
       "3               3         3              4  \n",
       "4               4         3              5  \n",
       "..            ...       ...            ...  \n",
       "95             30        30             96  \n",
       "96             97         5             97  \n",
       "97              -        32             98  \n",
       "98              -        71             99  \n",
       "99             75        37            100  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Top 100 Songs =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\")\n",
    "df = pd.DataFrame({'Song Name':song_name,\n",
    "                   'Artist Name':artist_name,\n",
    "                   'Last Week Rank':lastW_rank,\n",
    "                   'Peak Rank':peak_rank,\n",
    "                   'Weeks on Board':week_board\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d24644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02a1152",
   "metadata": {},
   "source": [
    "## 6. Scrape the details of Highest selling novels.\n",
    "    A) Book name\n",
    "    B) Author name\n",
    "    C) Volumes sold\n",
    "    D) Publisher\n",
    "    E) Genre\n",
    "Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46ed684",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12702f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Da Vinci Code,The', 'Harry Potter and the Deathly Hallows', \"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Order of the Phoenix', 'Fifty Shades of Grey', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Angels and Demons', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Fifty Shades Darker', 'Twilight', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Fifty Shades Freed', 'Lost Symbol,The', 'New Moon', 'Deception Point', 'Eclipse', 'Lovely Bones,The', 'Curious Incident of the Dog in the Night-time,The', 'Digital Fortress', 'Short History of Nearly Everything,A', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Breaking Dawn', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Gruffalo,The', \"Jamie's 30-Minute Meals\", 'Kite Runner,The', 'One Day', 'Thousand Splendid Suns,A', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", \"Time Traveler's Wife,The\", 'Atonement', \"Bridget Jones's Diary:A Novel\", 'World According to Clarkson,The', \"Captain Corelli's Mandolin\", 'Sound of Laughter,The', 'Life of Pi', 'Billy Connolly', 'Child Called It,A', \"Gruffalo's Child,The\", \"Angela's Ashes:A Memoir of a Childhood\", 'Birdsong', 'Northern Lights:His Dark Materials S.', 'Labyrinth', 'Harry Potter and the Half-blood Prince', 'Help,The', 'Man and Boy', 'Memoirs of a Geisha', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'Island,The', 'PS, I Love You', 'You are What You Eat:The Plan That Will Change Your Life', 'Shadow of the Wind,The', 'Tales of Beedle the Bard,The', 'Broker,The', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Subtle Knife,The:His Dark Materials S.', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', \"Delia's How to Cook:(Bk.1)\", 'Chocolat', 'Boy in the Striped Pyjamas,The', \"My Sister's Keeper\", 'Amber Spyglass,The:His Dark Materials S.', 'To Kill a Mockingbird', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Dear Fatty', 'Short History of Tractors in Ukrainian,A', 'Hannibal', 'Lord of the Rings,The', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Interpretation of Murder,The', 'Sharon Osbourne Extreme:My Autobiography', 'Alchemist,The:A Fable About Following Your Dream', \"At My Mother's Knee ...:and Other Low Joints\", 'Notes from a Small Island', 'Return of the Naked Chef,The', 'Bridget Jones: The Edge of Reason', \"Jamie's Italy\", 'I Can Make You Thin', 'Down Under', 'Summons,The', 'Small Island', 'Nigella Express', 'Brick Lane', \"Memory Keeper's Daughter,The\", 'Room on the Broom', 'About a Boy', 'My Booky Wook', 'God Delusion,The', '\"Beano\" Annual,The', 'White Teeth', 'House at Riverton,The', 'Book Thief,The', 'Nights of Rain and Stars', 'Ghost,The', 'Happy Days with the Naked Chef', 'Hunger Games,The:Hunger Games Trilogy', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]\n",
      "100 ['Brown, Dan', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'James, E. L.', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'Brown, Dan', 'Rowling, J.K.', 'James, E. L.', 'Meyer, Stephenie', 'Larsson, Stieg', 'James, E. L.', 'Brown, Dan', 'Meyer, Stephenie', 'Brown, Dan', 'Meyer, Stephenie', 'Sebold, Alice', 'Haddon, Mark', 'Brown, Dan', 'Bryson, Bill', 'Larsson, Stieg', 'Meyer, Stephenie', 'Carle, Eric', 'Donaldson, Julia', 'Oliver, Jamie', 'Hosseini, Khaled', 'Nicholls, David', 'Hosseini, Khaled', 'Larsson, Stieg', 'Niffenegger, Audrey', 'McEwan, Ian', 'Fielding, Helen', 'Clarkson, Jeremy', 'Bernieres, Louis de', 'Kay, Peter', 'Martel, Yann', 'Stephenson, Pamela', 'Pelzer, Dave', 'Donaldson, Julia', 'McCourt, Frank', 'Faulks, Sebastian', 'Pullman, Philip', 'Mosse, Kate', 'Rowling, J.K.', 'Stockett, Kathryn', 'Parsons, Tony', 'Golden, Arthur', 'McCall Smith, Alexander', 'Hislop, Victoria', 'Ahern, Cecelia', 'McKeith, Gillian', 'Zafon, Carlos Ruiz', 'Rowling, J.K.', 'Grisham, John', 'Atkins, Robert C.', 'Pullman, Philip', 'Truss, Lynne', 'Smith, Delia', 'Harris, Joanne', 'Boyne, John', 'Picoult, Jodi', 'Pullman, Philip', 'Lee, Harper', 'Gray, John', 'French, Dawn', 'Lewycka, Marina', 'Harris, Thomas', 'Tolkien, J. R. R.', 'Moore, Michael', 'Rubenfeld, Jed', 'Osbourne, Sharon', 'Coelho, Paulo', \"O'Grady, Paul\", 'Bryson, Bill', 'Oliver, Jamie', 'Fielding, Helen', 'Oliver, Jamie', 'McKenna, Paul', 'Bryson, Bill', 'Grisham, John', 'Levy, Andrea', 'Lawson, Nigella', 'Ali, Monica', 'Edwards, Kim', 'Donaldson, Julia', 'Hornby, Nick', 'Brand, Russell', 'Dawkins, Richard', '0', 'Smith, Zadie', 'Morton, Kate', 'Zusak, Markus', 'Binchy, Maeve', 'Harris, Robert', 'Oliver, Jamie', 'Collins, Suzanne', 'Pelzer, Dave', 'Oliver, Jamie']\n",
      "100 ['5,094,805', '4,475,152', '4,200,654', '4,179,479', '3,758,936', '3,583,215', '3,484,047', '3,377,906', '3,193,946', '2,950,264', '2,479,784', '2,315,405', '2,233,570', '2,193,928', '2,183,031', '2,152,737', '2,062,145', '2,052,876', '2,005,598', '1,979,552', '1,928,900', '1,852,919', '1,814,784', '1,787,118', '1,783,535', '1,781,269', '1,743,266', '1,629,119', '1,616,068', '1,583,992', '1,555,135', '1,546,886', '1,539,428', '1,508,205', '1,489,403', '1,352,318', '1,310,207', '1,310,176', '1,231,957', '1,217,712', '1,208,711', '1,204,058', '1,184,967', '1,181,503', '1,181,093', '1,153,181', '1,132,336', '1,130,802', '1,126,337', '1,115,549', '1,108,328', '1,107,379', '1,104,403', '1,092,349', '1,090,847', '1,087,262', '1,054,196', '1,037,160', '1,023,688', '1,015,956', '1,009,873', '1,004,414', '1,003,780', '1,002,314', '998,213', '992,846', '986,753', '986,115', '970,509', '967,466', '963,353', '962,515', '959,496', '956,114', '945,640', '931,312', '925,425', '924,695', '906,968', '905,086', '890,847', '869,671', '869,659', '862,602', '856,540', '845,858', '842,535', '828,215', '820,563', '816,907', '816,585', '815,586', '814,370', '809,641', '808,900', '807,311', '794,201', '792,187', '791,507', '791,095']\n",
      "100 ['Transworld', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Random House', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Transworld', 'Bloomsbury', 'Random House', 'Little, Brown Book', 'Quercus', 'Random House', 'Transworld', 'Little, Brown Book', 'Transworld', 'Little, Brown Book', 'Pan Macmillan', 'Random House', 'Transworld', 'Transworld', 'Quercus', 'Little, Brown Book', 'Penguin', 'Pan Macmillan', 'Penguin', 'Bloomsbury', 'Hodder & Stoughton', 'Bloomsbury', 'Quercus', 'Random House', 'Random House', 'Pan Macmillan', 'Penguin', 'Random House', 'Random House', 'Canongate', 'HarperCollins', 'Orion', 'Pan Macmillan', 'HarperCollins', 'Random House', 'Scholastic Ltd.', 'Orion', 'Bloomsbury', 'Penguin', 'HarperCollins', 'Random House', 'Little, Brown Book', 'Headline', 'HarperCollins', 'Penguin', 'Orion', 'Bloomsbury', 'Random House', 'Random House', 'Scholastic Ltd.', 'Profile Books Group', 'Random House', 'Transworld', 'Random House Childrens Books G', 'Hodder & Stoughton', 'Scholastic Ltd.', 'Random House', 'HarperCollins', 'Random House', 'Penguin', 'Random House', 'HarperCollins', 'Penguin', 'Headline', 'Little, Brown Book', 'HarperCollins', 'Transworld', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Transworld', 'Transworld', 'Random House', 'Headline', 'Random House', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Hodder & Stoughton', 'Transworld', 'D.C. Thomson', 'Penguin', 'Pan Macmillan', 'Transworld', 'Orion', 'Random House', 'Penguin', 'Scholastic Ltd.', 'Orion', 'Penguin']\n",
      "100 ['Crime, Thriller & Adventure', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Romance & Sagas', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Crime, Thriller & Adventure', \"Children's Fiction\", 'Romance & Sagas', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Romance & Sagas', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Popular Science', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Picture Books', 'Picture Books', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Humour: Collections & General', 'General & Literary Fiction', 'Autobiography: General', 'General & Literary Fiction', 'Biography: The Arts', 'Autobiography: General', 'Picture Books', 'Autobiography: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Science Fiction & Fantasy', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'Fitness & Diet', 'General & Literary Fiction', \"Children's Fiction\", 'Crime, Thriller & Adventure', 'Fitness & Diet', 'Young Adult Fiction', 'Usage & Writing Guides', 'Food & Drink: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Popular Culture & Media: General Interest', 'Autobiography: The Arts', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Science Fiction & Fantasy', 'Current Affairs & Issues', 'Crime, Thriller & Adventure', 'Autobiography: The Arts', 'General & Literary Fiction', 'Autobiography: The Arts', 'Travel Writing', 'Food & Drink: General', 'General & Literary Fiction', 'National & Regional Cuisine', 'Fitness & Diet', 'Travel Writing', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'Picture Books', 'General & Literary Fiction', 'Autobiography: The Arts', 'Popular Science', \"Children's Annuals\", 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Food & Drink: General', 'Young Adult Fiction', 'Biography: General', 'Food & Drink: General']\n"
     ]
    }
   ],
   "source": [
    "book_name = []\n",
    "author_name = []\n",
    "volumes_sold = []\n",
    "publisher = []\n",
    "genre = []\n",
    "\n",
    "book_tags = driver.find_elements(By.XPATH,'//*[@class=\"in-article sortable\"]/tbody/tr/td[2]')\n",
    "for i in book_tags:\n",
    "    try:\n",
    "        book_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        book_name.append('-')\n",
    "print(len(book_name),book_name)\n",
    "\n",
    "author_tags = driver.find_elements(By.XPATH,'//*[@class=\"in-article sortable\"]/tbody/tr/td[3]')\n",
    "for i in author_tags:\n",
    "    try:\n",
    "        author_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        author_name.append('-')\n",
    "print(len(author_name),author_name)\n",
    "\n",
    "volumes_tags = driver.find_elements(By.XPATH,'//*[@class=\"in-article sortable\"]/tbody/tr/td[4]')\n",
    "for i in volumes_tags:\n",
    "    try:\n",
    "        volumes_sold.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        volumes_sold.append('-')\n",
    "print(len(volumes_sold),volumes_sold)\n",
    "\n",
    "publisher_tags = driver.find_elements(By.XPATH,'//*[@class=\"in-article sortable\"]/tbody/tr/td[5]')\n",
    "for i in publisher_tags:\n",
    "    try:\n",
    "        publisher.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        publisher.append('-')\n",
    "print(len(publisher),publisher)\n",
    "\n",
    "genre_tags = driver.find_elements(By.XPATH,'//*[@class=\"in-article sortable\"]/tbody/tr/td[6]')\n",
    "for i in genre_tags:\n",
    "    try:\n",
    "        genre.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        genre.append('-')\n",
    "print(len(genre),genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9500275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Highest Selling Novels =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volumes Sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name       Author Name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes Sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*= Highest Selling Novels =*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\")\n",
    "novel = pd.DataFrame({'Book Name':book_name,\n",
    "                      'Author Name':author_name,\n",
    "                      'Volumes Sold':volumes_sold,\n",
    "                      'Publisher':publisher,\n",
    "                      'Genre':genre\n",
    "                     })\n",
    "novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f82da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6941a998",
   "metadata": {},
   "source": [
    "## 7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "    \n",
    "    A) Name\n",
    "    B) Year span\n",
    "    C) Genre\n",
    "    D) Run time\n",
    "    E) Ratings\n",
    "    F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a3e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls512407256/\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686b27c",
   "metadata": {},
   "source": [
    "menu = driver.find_element(By.XPATH,'//label[@id=\"imdbHeader-navDrawerOpen\"]/span').click()\n",
    "sleep(4)\n",
    "popular = driver.find_element(By.XPATH,'//div[@data-testid=\"grouped-link-category\"]/div[1]/span/div/div/ul/a[3]').click()\n",
    "slepp(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415749c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Game of Thrones', 'Stranger Things', 'The Walking Dead', '13 Reasons Why', 'The 100', 'Orange Is the New Black', 'Riverdale', \"Grey's Anatomy\", 'The Flash', 'Arrow', 'Money Heist', 'The Big Bang Theory', 'Black Mirror', 'Sherlock', 'Vikings', 'Pretty Little Liars', 'The Vampire Diaries', 'American Horror Story', 'Breaking Bad', 'Lucifer', 'Supernatural', 'Prison Break', 'How to Get Away with Murder', 'Teen Wolf', 'The Simpsons', 'Once Upon a Time', 'Narcos', 'Daredevil', 'Friends', 'How I Met Your Mother', 'Suits', 'Mr. Robot', 'The Originals', 'Supergirl', 'Gossip Girl', 'Sense8', 'Gotham', 'Westworld', 'Jessica Jones', 'Modern Family', 'Rick and Morty', 'Shadowhunters', 'The End of the F***ing World', 'House of Cards', 'Dark', 'Elite', 'Sex Education', 'Shameless', 'New Girl', 'Agents of S.H.I.E.L.D.', 'Game of Thrones', 'You', 'Stranger Things', 'Dexter', 'The Walking Dead', 'Fear the Walking Dead', '13 Reasons Why', 'Family Guy', 'The 100', 'The Blacklist', 'Orange Is the New Black', 'Lost', 'Riverdale', 'Peaky Blinders', \"Grey's Anatomy\", 'House', 'The Flash', 'Quantico', 'Arrow', 'Orphan Black', 'Money Heist', 'Homeland', 'The Big Bang Theory', 'Blindspot', 'Black Mirror', \"DC's Legends of Tomorrow\", 'Sherlock', \"The Handmaid's Tale\", 'Vikings', 'Chilling Adventures of Sabrina', 'Pretty Little Liars', 'The Good Doctor', 'The Vampire Diaries', 'Jane the Virgin', 'American Horror Story', 'Glee', 'Breaking Bad', 'South Park', 'Lucifer', 'Brooklyn Nine-Nine', 'Supernatural', 'Under the Dome', 'Prison Break', 'The Umbrella Academy', 'How to Get Away with Murder', 'True Detective', 'Teen Wolf', 'The OA', 'The Simpsons', 'Desperate Housewives']\n",
      "100 ['(2011–2019)', '(2016–2025)', '(2010–2022)', '(2017–2020)', '(2014–2020)', '(2013–2019)', '(2017–2023)', '(2005– )', '(2014–2023)', '(2012–2020)', '(2017–2021)', '(2007–2019)', '(2011– )', '(2010–2017)', '(2013–2020)', '(2010–2017)', '(2009–2017)', '(2011– )', '(2008–2013)', '(2016–2021)', '(2005–2020)', '(2005–2017)', '(2014–2020)', '(2011–2017)', '(1989– )', '(2011–2018)', '(I) (2015–2017)', '(2015–2018)', '(1994–2004)', '(2005–2014)', '(2011–2019)', '(2015–2019)', '(2013–2018)', '(2015–2021)', '(2007–2012)', '(2015–2018)', '(2014–2019)', '(2016–2022)', '(2015–2019)', '(2009–2020)', '(2013– )', '(2016–2019)', '(2017–2019)', '(2013–2018)', '(2017–2020)', '(2018–2024)', '(2019–2023)', '(2011–2021)', '(2011–2018)', '(2013–2020)', '(2011–2019)', '(2018–2024)', '(2016–2025)', '(2006–2013)', '(2010–2022)', '(2015–2023)', '(2017–2020)', '(1999–2025)', '(2014–2020)', '(2013–2023)', '(2013–2019)', '(2004–2010)', '(2017–2023)', '(2013–2022)', '(2005– )', '(2004–2012)', '(2014–2023)', '(2015–2018)', '(2012–2020)', '(2013–2017)', '(2017–2021)', '(2011–2020)', '(2007–2019)', '(2015–2020)', '(2011– )', '(2016–2022)', '(2010–2017)', '(2017– )', '(2013–2020)', '(2018–2020)', '(2010–2017)', '(2017–2024)', '(2009–2017)', '(2014–2019)', '(2011– )', '(2009–2015)', '(2008–2013)', '(1997– )', '(2016–2021)', '(2013–2021)', '(2005–2020)', '(2013–2015)', '(2005–2017)', '(2019–2024)', '(2014–2020)', '(2014– )', '(2011–2017)', '(2016–2019)', '(1989– )', '(2004–2012)']\n",
      "100 ['Action, Adventure, Drama', 'Drama, Fantasy, Horror', 'Drama, Horror, Thriller', 'Drama, Mystery, Thriller', 'Drama, Mystery, Sci-Fi', 'Comedy, Crime, Drama', 'Crime, Drama, Mystery', 'Drama, Romance', 'Action, Adventure, Drama', 'Action, Adventure, Crime', 'Action, Crime, Drama', 'Comedy, Romance', 'Drama, Mystery, Sci-Fi', 'Crime, Drama, Mystery', 'Action, Adventure, Drama', 'Drama, Mystery, Romance', 'Drama, Fantasy, Horror', 'Drama, Horror, Sci-Fi', 'Crime, Drama, Thriller', 'Crime, Drama, Fantasy', 'Drama, Fantasy, Horror', 'Action, Crime, Drama', 'Crime, Drama, Mystery', 'Action, Drama, Fantasy', 'Animation, Comedy', 'Adventure, Fantasy, Romance', 'Biography, Crime, Drama', 'Action, Crime, Drama', 'Comedy, Romance', 'Comedy, Drama, Romance', 'Comedy, Drama', 'Crime, Drama, Thriller', 'Drama, Fantasy, Horror', 'Action, Adventure, Drama', 'Drama, Romance', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Comedy, Drama, Romance', 'Animation, Adventure, Comedy', 'Action, Drama, Fantasy', 'Adventure, Comedy, Crime', 'Drama', 'Crime, Drama, Mystery', 'Crime, Drama, Thriller', 'Comedy, Drama, Romance', 'Comedy, Drama', 'Comedy, Romance', 'Action, Adventure, Drama', 'Action, Adventure, Drama', 'Crime, Drama, Romance', 'Drama, Fantasy, Horror', 'Crime, Drama, Mystery', 'Drama, Horror, Thriller', 'Drama, Horror, Sci-Fi', 'Drama, Mystery, Thriller', 'Animation, Comedy', 'Drama, Mystery, Sci-Fi', 'Crime, Drama, Mystery', 'Comedy, Crime, Drama', 'Adventure, Drama, Fantasy', 'Crime, Drama, Mystery', 'Crime, Drama', 'Drama, Romance', 'Drama, Mystery', 'Action, Adventure, Drama', 'Crime, Drama, Mystery', 'Action, Adventure, Crime', 'Drama, Sci-Fi, Thriller', 'Action, Crime, Drama', 'Crime, Drama, Mystery', 'Comedy, Romance', 'Action, Crime, Drama', 'Drama, Mystery, Sci-Fi', 'Action, Adventure, Drama', 'Crime, Drama, Mystery', 'Drama, Sci-Fi, Thriller', 'Action, Adventure, Drama', 'Drama, Fantasy, Horror', 'Drama, Mystery, Romance', 'Drama', 'Drama, Fantasy, Horror', 'Comedy', 'Drama, Horror, Sci-Fi', 'Comedy, Drama, Music', 'Crime, Drama, Thriller', 'Animation, Comedy', 'Crime, Drama, Fantasy', 'Comedy, Crime', 'Drama, Fantasy, Horror', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Action, Adventure, Comedy', 'Crime, Drama, Mystery', 'Crime, Drama, Mystery', 'Action, Drama, Fantasy', 'Drama, Fantasy, Mystery', 'Animation, Comedy', 'Comedy, Drama, Mystery']\n",
      "100 ['55 min', '51 min', '44 min', '60 min', '43 min', '59 min', '45 min', '41 min', '43 min', '42 min', '70 min', '22 min', '60 min', '88 min', '44 min', '44 min', '43 min', '60 min', '45 min', '4,393 min', '44 min', '44 min', '43 min', '41 min', '22 min', '60 min', '49 min', '54 min', '5,280 min', '4,576 min', '44 min', '49 min', '45 min', '43 min', '42 min', '60 min', '42 min', '62 min', '56 min', '5,378 min', '23 min', '42 min', '25 min', '3,804 min', '1,455 min', '60 min', '1,755 min', '46 min', '22 min', '45 min', '55 min', '45 min', '51 min', '55 min', '44 min', '44 min', '60 min', '22 min', '43 min', '43 min', '59 min', '5,445 min', '45 min', '2,109 min', '41 min', '7,788 min', '43 min', '42 min', '42 min', '44 min', '70 min', '55 min', '22 min', '42 min', '60 min', '42 min', '88 min', '60 min', '44 min', '60 min', '44 min', '41 min', '43 min', '60 min', '60 min', '44 min', '45 min', '22 min', '4,393 min', '22 min', '44 min', '43 min', '44 min', '60 min', '43 min', '55 min', '41 min', '60 min', '22 min', '45 min']\n",
      "100 ['9.2', '8.7', '8.1', '7.5', '7.6', '8', '6.5', '7.6', '7.5', '7.5', '8.2', '8.1', '8.7', '9.1', '8.5', '7.4', '7.7', '8', '9.5', '8.1', '8.4', '8.3', '8.1', '7.7', '8.7', '7.7', '8.8', '8.6', '8.9', '8.3', '8.4', '8.5', '8.3', '6.2', '7.5', '8.2', '7.8', '8.5', '7.9', '8.5', '9.1', '6.5', '8', '8.6', '8.7', '7.2', '8.3', '8.5', '7.8', '7.5', '9.2', '7.7', '8.7', '8.7', '8.1', '6.8', '7.5', '8.1', '7.6', '7.9', '8', '8.3', '6.5', '8.8', '7.6', '8.7', '7.5', '6.7', '7.5', '8.3', '8.2', '8.3', '8.1', '7.3', '8.7', '6.8', '9.1', '8.4', '8.5', '7.4', '7.4', '8', '7.7', '7.9', '8', '6.8', '9.5', '8.7', '8.1', '8.4', '8.4', '6.5', '8.3', '7.9', '8.1', '8.9', '7.7', '7.8', '8.7', '7.6']\n",
      "100 ['2,261,972', '1,320,083', '1,071,953', '313,429', '273,342', '318,654', '154,353', '338,267', '366,922', '444,740', '526,687', '861,091', '633,236', '989,336', '576,029', '177,349', '347,062', '341,269', '2,110,055', '352,697', '479,067', '574,819', '165,321', '162,029', '433,029', '235,942', '465,788', '471,296', '1,078,708', '723,955', '472,201', '415,140', '146,689', '129,116', '191,513', '162,174', '239,825', '529,650', '225,666', '477,230', '594,670', '69,860', '217,281', '528,318', '437,708', '89,856', '346,847', '279,833', '243,700', '225,144', '2,261,972', '295,823', '1,320,083', '759,510', '1,071,953', '142,826', '313,429', '361,910', '273,342', '277,232', '318,654', '590,309', '154,353', '637,593', '338,267', '505,434', '366,922', '63,656', '444,740', '116,459', '526,687', '359,256', '861,091', '78,613', '633,236', '109,668', '989,336', '255,904', '576,029', '107,372', '177,349', '110,817', '347,062', '58,387', '341,269', '154,871', '2,110,055', '403,332', '352,697', '356,240', '479,067', '111,946', '574,819', '272,443', '165,321', '645,034', '162,029', '114,842', '433,029', '138,659']\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "years = []\n",
    "genres = []\n",
    "runtime = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "name_tags = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/h3/a')\n",
    "for i in name_tags:\n",
    "    try:\n",
    "        names.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        names.append('-')\n",
    "print(len(names),names)\n",
    "\n",
    "year_tags = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/h3/span[2]')\n",
    "for i in year_tags:\n",
    "    try:\n",
    "        years.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        years.append('-')\n",
    "print(len(years),years)\n",
    "\n",
    "genre_tags = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/p/span[5]')\n",
    "for i in genre_tags:\n",
    "    try:\n",
    "        genres.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        genres.append('-')\n",
    "print(len(genres),genres)\n",
    "\n",
    "runtime_tags = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/p/span[3]')\n",
    "for i in runtime_tags:\n",
    "    try:\n",
    "        runtime.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        runtime.append('-')\n",
    "print(len(runtime),runtime)\n",
    "\n",
    "rating_tags = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/div/div/span[2]')\n",
    "for i in rating_tags:\n",
    "    try:\n",
    "        ratings.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        ratings.append('-')\n",
    "print(len(ratings),ratings)\n",
    "\n",
    "votes_tags = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/p[4]/span[2]')\n",
    "for i in votes_tags:\n",
    "    try:\n",
    "        votes.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        votes.append('-')\n",
    "print(len(votes),votes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "095266ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=*=*=*=*=*=*=*= Top 100 most watched tv shows of all time =*=*=*=*=*=*=*=\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>55 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,261,972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2025)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,320,083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,071,953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>313,429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>273,342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>True Detective</td>\n",
       "      <td>(2014– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>55 min</td>\n",
       "      <td>8.9</td>\n",
       "      <td>645,034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Teen Wolf</td>\n",
       "      <td>(2011–2017)</td>\n",
       "      <td>Action, Drama, Fantasy</td>\n",
       "      <td>41 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>162,029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The OA</td>\n",
       "      <td>(2016–2019)</td>\n",
       "      <td>Drama, Fantasy, Mystery</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>114,842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The Simpsons</td>\n",
       "      <td>(1989– )</td>\n",
       "      <td>Animation, Comedy</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>433,029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Desperate Housewives</td>\n",
       "      <td>(2004–2012)</td>\n",
       "      <td>Comedy, Drama, Mystery</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>138,659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name    Year Span                     Genre Run Time  \\\n",
       "0        Game of Thrones  (2011–2019)  Action, Adventure, Drama   55 min   \n",
       "1        Stranger Things  (2016–2025)    Drama, Fantasy, Horror   51 min   \n",
       "2       The Walking Dead  (2010–2022)   Drama, Horror, Thriller   44 min   \n",
       "3         13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   60 min   \n",
       "4                The 100  (2014–2020)    Drama, Mystery, Sci-Fi   43 min   \n",
       "..                   ...          ...                       ...      ...   \n",
       "95        True Detective     (2014– )     Crime, Drama, Mystery   55 min   \n",
       "96             Teen Wolf  (2011–2017)    Action, Drama, Fantasy   41 min   \n",
       "97                The OA  (2016–2019)   Drama, Fantasy, Mystery   60 min   \n",
       "98          The Simpsons     (1989– )         Animation, Comedy   22 min   \n",
       "99  Desperate Housewives  (2004–2012)    Comedy, Drama, Mystery   45 min   \n",
       "\n",
       "   Rating      Votes  \n",
       "0     9.2  2,261,972  \n",
       "1     8.7  1,320,083  \n",
       "2     8.1  1,071,953  \n",
       "3     7.5    313,429  \n",
       "4     7.6    273,342  \n",
       "..    ...        ...  \n",
       "95    8.9    645,034  \n",
       "96    7.7    162,029  \n",
       "97    7.8    114,842  \n",
       "98    8.7    433,029  \n",
       "99    7.6    138,659  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=*=*=*=*=*=*=*= Top 100 most watched tv shows of all time =*=*=*=*=*=*=*=\")\n",
    "TV_Series = pd.DataFrame({'Name':names,\n",
    "                          'Year Span':years,\n",
    "                          'Genre':genres,\n",
    "                          'Run Time':runtime,\n",
    "                          'Rating':ratings,\n",
    "                          'Votes':votes\n",
    "                         })\n",
    "TV_Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9486237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b23b709",
   "metadata": {},
   "source": [
    "## 8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "\n",
    "    A) Dataset name\n",
    "    B) Data type\n",
    "    C) Task\n",
    "    D) Attribute type\n",
    "    E) No of instances\n",
    "    F) No of attribute\n",
    "    G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9a44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371dbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/header/nav/ul/li[1]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3d8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "Dtype = []\n",
    "task = []\n",
    "Atype = []\n",
    "instances = []\n",
    "no_attr = []\n",
    "#year = [] # Need Expand Button-It iterate only for first page but not for all (not get proper loop to iterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7a85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range (0,66):\n",
    "    next_page = driver.find_element(By.XPATH,'//button[@class=\"btn-primary btn-sm btn\"][2]').click()\n",
    "    sleep(3)\n",
    "    \n",
    "    name_tags = driver.find_elements(By.XPATH,'//h2[@class=\"truncate text-primary\"]')\n",
    "    for i in name_tags:                       \n",
    "        try:\n",
    "            name.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            name.append('-')\n",
    "    Dtype_tags = driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]//div[1]/span')\n",
    "    for i in Dtype_tags:\n",
    "        try:\n",
    "            Dtype.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            Dtype.append('-')\n",
    "    task_tags = driver.find_elements(By.XPATH,'//p[@class=\"truncate\"]')\n",
    "    for i in task_tags:\n",
    "        try:\n",
    "            task.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            task.append('-')\n",
    "    Atype_tags = driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]//div[2]/span')\n",
    "    for i in Atype_tags:\n",
    "        try:\n",
    "            Atype.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            Atype.append('-')\n",
    "    instances_tags = driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]//div[3]/span')\n",
    "    for i in instances_tags:\n",
    "        try:\n",
    "            instances.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            instances.append('-')\n",
    "    no_attr_tags = driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]//div[4]/span')\n",
    "    for i in no_attr_tags:\n",
    "        try:\n",
    "            no_attr.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            no_attr.append('-') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e2519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654 654 654 654 654 654\n"
     ]
    }
   ],
   "source": [
    "print(len(name),\n",
    "      len(Dtype),\n",
    "      len(task),\n",
    "      len(Atype),\n",
    "      len(instances),\n",
    "      len(no_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d01e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instances</th>\n",
       "      <th>No of Attribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Car Evaluation</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Derived from simple hierarchical decision mode...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>1.73K Instances</td>\n",
       "      <td>6 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bank Marketing</td>\n",
       "      <td>Classification</td>\n",
       "      <td>The data is related with direct marketing camp...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>45.21K Instances</td>\n",
       "      <td>17 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mushroom</td>\n",
       "      <td>Classification</td>\n",
       "      <td>From Audobon Society Field Guide; mushrooms de...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>8.12K Instances</td>\n",
       "      <td>22 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Predict the age of abalone from physical measu...</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>4.18K Instances</td>\n",
       "      <td>8 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student Performance</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Predict student performance in secondary educa...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>649 Instances</td>\n",
       "      <td>33 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>NSF Research Award Abstracts 1990-2003</td>\n",
       "      <td></td>\n",
       "      <td>This data set consists of (a) 129,000 abstract...</td>\n",
       "      <td></td>\n",
       "      <td>129K Instances</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>PMU-UD</td>\n",
       "      <td>Classification</td>\n",
       "      <td>The handwritten dataset was collected from 170...</td>\n",
       "      <td>Univariate</td>\n",
       "      <td>5.18K Instances</td>\n",
       "      <td>9 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>DGP2 - The Second Data Generation Program</td>\n",
       "      <td></td>\n",
       "      <td>Generates application domains based on specifi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>EBL Domain Theories</td>\n",
       "      <td></td>\n",
       "      <td>Assorted small-scale domain theories</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>Sattriya_Dance_Single_Hand_Gestures Dataset</td>\n",
       "      <td>Classification</td>\n",
       "      <td>The Sattriya_Dance_Single_Hand_Gestures datase...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>1.45K Instances</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>654 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Dataset Name                   Data Type  \\\n",
       "0                                 Car Evaluation              Classification   \n",
       "1                                 Bank Marketing              Classification   \n",
       "2                                       Mushroom              Classification   \n",
       "3                                        Abalone  Classification, Regression   \n",
       "4                            Student Performance  Classification, Regression   \n",
       "..                                           ...                         ...   \n",
       "649       NSF Research Award Abstracts 1990-2003                               \n",
       "650                                       PMU-UD              Classification   \n",
       "651    DGP2 - The Second Data Generation Program                               \n",
       "652                          EBL Domain Theories                               \n",
       "653  Sattriya_Dance_Single_Hand_Gestures Dataset              Classification   \n",
       "\n",
       "                                                  Task Attribute Type  \\\n",
       "0    Derived from simple hierarchical decision mode...   Multivariate   \n",
       "1    The data is related with direct marketing camp...   Multivariate   \n",
       "2    From Audobon Society Field Guide; mushrooms de...   Multivariate   \n",
       "3    Predict the age of abalone from physical measu...        Tabular   \n",
       "4    Predict student performance in secondary educa...   Multivariate   \n",
       "..                                                 ...            ...   \n",
       "649  This data set consists of (a) 129,000 abstract...                  \n",
       "650  The handwritten dataset was collected from 170...     Univariate   \n",
       "651  Generates application domains based on specifi...                  \n",
       "652               Assorted small-scale domain theories                  \n",
       "653  The Sattriya_Dance_Single_Hand_Gestures datase...   Multivariate   \n",
       "\n",
       "      No of Instances No of Attribute  \n",
       "0     1.73K Instances      6 Features  \n",
       "1    45.21K Instances     17 Features  \n",
       "2     8.12K Instances     22 Features  \n",
       "3     4.18K Instances      8 Features  \n",
       "4       649 Instances     33 Features  \n",
       "..                ...             ...  \n",
       "649    129K Instances                  \n",
       "650   5.18K Instances      9 Features  \n",
       "651                                    \n",
       "652                                    \n",
       "653   1.45K Instances                  \n",
       "\n",
       "[654 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = pd.DataFrame({'Dataset Name':name,\n",
    "                         'Data Type':Dtype,\n",
    "                         'Task':task,\n",
    "                         'Attribute Type':Atype,\n",
    "                         'No of Instances':instances,\n",
    "                         'No of Attribute':no_attr\n",
    "                        })\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdae14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e7e18e3",
   "metadata": {},
   "source": [
    "# End of Assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
